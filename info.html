<!DOCTYPE html>
<html lang="en">

<head>
  <link rel="icon" href="favicon.webp" type="image/x-icon">
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Demos - Visual Intelligence Interest Group (VIIG)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="css/fotter.css">
  <link rel="stylesheet" href="css/stanford.css">

  <style type="text/css">
    /* ul {
      margin-left: 20px;
    } */

    p {
      text-align: justify;
    }

    table {
      margin-left: auto;
      margin-right: auto;
    }

    .container {
      width: 100%;
    }

    video {
      width: 100%;
      height: auto;
    }

    .hamburger {
      position: relative;
      cursor: pointer;
      z-index: 1000;
    }

    #navigation {
      position: absolute;
      top: 100%;
      left: -40px;
      width: 200%;
      box-sizing: border-box;
      background-color: rgba(255, 255, 255);
      padding: 0;
      margin: 0;
      list-style: none;
      display: flex;
      flex-direction: column;
      align-items: center;
      opacity: 0;
      visibility: hidden;
      transition: opacity 0.3s, visibility 0.3s;
      box-shadow: none;
    }

    #navigation.show-menu {
      opacity: 1;
      visibility: visible;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
    }
  </style>
</head>

<body class='page page-about'>
  <div class="wrapper">
    <div class='header'>
      <div class="container">
        <h3 class="header-title">
          <a href="./index.html"><img src="./grouplogo.webp" style="max-width: 50%"></a>
        </h3>
        <div id="main-menu" class="main-menu">
          <ul>
            <li class="menu-item-about">
              <a href="./index.html">
                <span>Home</span>
              </a>
            </li>
            <li class="menu-item-people">
              <a href="./people.html">
                <span>People</span>
              </a>
            </li>
            <li class="menu-item-publications">
              <a href="./publication.html">
                <span>Publication</span>
              </a>
            </li>
            <li class="menu-item-software">
              <a href="./research.html">
                <span>Research</span>
              </a>
            </li>
            <li class="menu-item-software  active">
              <a href="./info.html">
                <span>Demos</span>
              </a>
            </li>
          </ul>
        </div>
        <div class="hamburger hamburger--slider" tabindex="0" aria-label="Menu" role="button"
          aria-controls="navigation">
          <div class="hamburger-box">
            <div class="hamburger-inner"></div>
          </div>
          <nav id="navigation">
            <a href="./index.html">
              <span>Home</span>
            </a>
            <a href="./people.html">
              <span>People</span>
            </a>
            <a href="./publication.html">
              <span>Publication</span>
            </a>
            <a href="./research.html">
              <span>Research</span>
            </a>
            <a href="./info.html">
              <span>Demos</span>
            </a>
          </nav>
        </div>
      </div>
    </div>

    <div class="container mt-2">
      <div>
        <ul>
          <ul>
            <h3>GOT-10k: A Large High-diversity Benchmark and Evaluation Platform for Single Object Tracking</h3>
            <p>
              Visual Object Tracking & Evaluation Technology & Large High-diversity Benchmark
            </p>
            <p>
              GOT-10k is constructed to evaluate the generalization ability of trackers on unseen object classes and
              motion patterns. The platform provides a high-quality video trajectory dataset containing 10,000 video
              segments, 563 object classes, 87 motion patterns, and 1.5 million tight annotations, where its coverage of
              object classes is magnitudes wider than other existing tracking benchmarks.
            </p>
            <p>
              GOT-10k is the supporting platform for a research accepted by IEEE TPAMI. It receives 3.24m page views,
              6.7k+ downloads, 17.9k+ trackers from 150+ countries(statistics by Feb. 2024).
            </p>
          </ul>
        </ul>
        <table>
          <tr>
            <td>
              <video width="1000" controls="true" controlslist="nodownload">
                <source src="img/demos/got-10k.mp4" type="video/mp4">
                </source>
              </video>
            </td>
          </tr>
        </table>
      </div>
      <br />
      <br />
      <div>
        <ul>
          <ul>
            <h3>VideoCube: A Large-scale Multi-dimensional Global Instance Tracking Intelligent Evaluation Platform</h3>
            <p>
              Visual Object Tracking & Large-scale Benchmark Construction & Intelligent Evaluation Technology
            </p>
            <p>
              This work builds upon the concept of human-like modeling and expands the defi
              nition of single object tracking (SOT) task. It presents a new task called global instance
              tracking (GIT), which broadens the range of applications to adversarial scenarios.
              This work proposes a video narrative content decoupling framework based on film
              theory, and builds a large-scale, multi-dimensional global instance tracking task intel
              ligent evaluation platform called VideoCube, which includes 7.46 million video frames.
              It is currently the largest SOT benchmark in terms of scale.
              Starting from human-computer confrontation, for the first time, human subjects
              are introduced into the SOT task in order to measure the visual tracking intelligence.
            </p>
            <p>
              This work has been published by IEEE TPAMI (IF=23.6) in Jan. 2023. As of
              Feb. 2024, the platform has received over 315k visits from more than 130 countries and
              regions worldwide, with over 1,000 downloads and more than 400 algorithm tests.
            </p>
          </ul>
        </ul>
        <table>
          <tr>
            <td>
              <video width="1000" controls="true" controlslist="nodownload">
                <source src="img/demos/videocube.mp4" type="video/mp4">
                </source>
              </video>
            </td>
          </tr>
        </table>
      </div>
      <br />
      <br />
      <div>
        <ul>
          <ul>
            <h3>MGIT: A Multi-modal Global Instance Tracking Benchmark Based on Hierarchical Semantic Framework</h3>
            <p>
              Visual Language Tracking & Long Video Understanding and Reasoning & Hierar
              chical Semantic Information Annotation
            </p>
            <p>
              This work extends the GIT task and the VideoCube benchmark by constructing a
              multi-modal benchmark called MGIT. The MGIT benchmark is designed to capture
              the complex video narrative relationships and fully encompass the intricate spatio
              temporal and causal connections illustrated in long videos.
              This work introduces an innovative multi-granularity semantic information anno
              tation strategy by incorporating the hierarchical structure of human cognition. The
              strategy aims to provide high-quality semantic information and its effectiveness is val
              idated through experiments.
              This work introduces an innovative multi-granularity semantic information anno
              tation strategy by incorporating the hierarchical structure of human cognition. The
              strategy aims to provide high-quality semantic information and its effectiveness is val
              idated through experiments.
            </p>
            <p>
              This work has been accepted by NeurIPS in Sep. 2023.
            </p>
          </ul>
        </ul>
        <table>
          <tr>
            <td>
              <video width="1000" controls="true" controlslist="nodownload">
                <source src="img/demos/mgit.mp4" type="video/mp4">
                </source>
              </video>
            </td>
          </tr>
        </table>
      </div>
      <br />
      <br />
      <div>
        <ul>
          <ul>
            <h3>BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for Ro
              bust Vision</h3>
            <p>
              Visual Object Tracking & Drone-based Tracking & Robust Visual Research
            </p>
            <p>
              BioDrone is the first bionic drone-based SOT benchmark, it features videos cap
              tured from a flapping-wing UAV system with a major camera shake due to its aerody
              namics. BioDrone highlights the tracking of tiny targets with drastic changes between
              consecutive frames, providing a new robust vision benchmark for SOT.
              Twenty representative algorithms have been replicated and tested on BioDrone,
              followed by a detailed analysis of the robustness bottlenecks.
              Abaseline algorithm namedUAV-KThasbeendesigned. Experimentalanalysishas
              beenconductedtovalidate the effectiveness of this method in extracting visual features
              and maintaining robustness in challenging factors present in UAV scenes.
            </p>
            <p>
              This work supported the organization of the 3rd High-Speed Low-Power Visual
              Understanding Challenge as competition data from May to Oct. 2022. The work has
              been accepted by IJCV (IF=19.5) in Oct. 2023
            </p>
          </ul>
        </ul>
        <table>
          <tr>
            <td>
              <video width="1000" controls="true" controlslist="nodownload">
                <source src="img/demos/biodrone.mp4" type="video/mp4">
                </source>
              </video>
            </td>
          </tr>
        </table>
      </div>
      <br />
      <br />
      <div>
        <ul>
          <ul>
            <h3>AWCV-100k: A Unconstrained Air-writing Benchmark for Real-World Applications</h3>
            <p>
              Air-writing Technique & Benchmark Construction & Human-machine Interaction
            </p>
            <p>
              This study has developed a large-scale and high-quality video dataset named AWCV-100k, which consists of
              air-writing of Chinese characters. The objective is to establish a more natural and comprehensive
              experimental environment for human-computer interaction (HCI) research.
              The AWCV-100k dataset comprises 8.8 million video frames, encompassing diverse environmental settings and
              lighting conditions. It provides comprehensive coverage of 3,755 Chinese characters from the GB2312-80
              character set, establishing it as the most extensive and comprehensive air-writing video dataset currently
              accessible.
              An air-writing character recognition algorithm called VCRec is proposed. This baseline algorithm is
              capable of extracting fingertip features from sparse visual cues and analyzing them using a
              spatio-temporal sequence module. Representative algorithms and VCRec have been reproduced and tested on
              the AWCV-100k. Experimental results confirm the robustness and effectiveness of VCRec.
            </p>
            <p>
              This work has been accepted by IEEE TCSVT (IF=8.4) in Apr. 2024. Subsequent work will be conducted
              based on this benchmark for HCI technology research.
            </p>
          </ul>
        </ul>
        <table>
          <tr>
            <td>
              <video width="1000" controls="true" controlslist="nodownload">
                <source src="img/demos/awcv.mp4" type="video/mp4">
                </source>
              </video>
            </td>
          </tr>
        </table>
      </div>
    </div>
  </div>


  <script>
    document.addEventListener('DOMContentLoaded', function () {
      var menuButton = document.querySelector('.hamburger');
      var navigation = document.getElementById('navigation');

      menuButton.addEventListener('click', function () {
        navigation.classList.toggle('show-menu');
      });
    });
  </script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/jquery-3.4.1.slim.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.15.0/umd/popper.min.js"
    integrity="sha256-fTuUgtT7O2rqoImwjrhDgbXTKUwyxxujIMRIK7TbuNU=" crossorigin="anonymous"></script>
  <!-- <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
    integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
    crossorigin="anonymous"></script>
  <script>
    $(function () { $('[data-toggle="tooltip"]').tooltip() })
  </script> -->
</body>

</html>