<!DOCTYPE html>
<html lang="en">

<head>
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-HJJPG4QHJ9"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-HJJPG4QHJ9');
  </script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="favicon.webp" type="image/x-icon">
  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Research - Visual Intelligence Interest Group (VIIG)</title>
  <link rel="stylesheet" href="css/stanford.css">
  <style type="text/css">
    .icon-with-text {
      display: inline-block;
      /* 使容器内的内容水平排列 */
      position: relative;
      /* 相对定位，用于绝对定位文字 */
      margin-left: 20px;
      /* 根据需要调整，以在图标和文字之间留出空间 */
    }

    .icon-with-text img {
      width: 100%;
      /* 图标宽度，可根据需要调整 */
    }

    .icon-with-text .text {
      position: absolute;
      /* 文字绝对定位 */
      left: 160px;
      /* 文字相对于图标的水平位置，根据图标大小调整 */
      top: 0;
      /* 文字相对于图标的垂直位置 */
      width: calc(100%);
      /* 根据需要调整，以适应多行文字 */
    }

    .icon-with-text .text h6 {
      margin: 0;
      /* 移除段落的默认外边距 */
      line-height: 1.25;
      /* 根据需要调整行高 */
    }

    ul {
      margin-left: 20px;
    }

    .media {
      border-bottom: solid 1px #E4E4E4;
      margin-bottom: 10px;
      margin-top: 10px;
    }

    .media .img-fluid {
      float: left;
      margin-right: 10px;
      margin-bottom: 10px;
    }

    .media .body {
      padding-left: 220px;
      text-align: justify;
    }

    p {
      text-align: justify;
    }

    .hamburger {
      position: relative;
      cursor: pointer;
      z-index: 1000;
    }

    #navigation {
      position: absolute;
      top: 100%;
      left: -40px;
      width: 200%;
      box-sizing: border-box;
      background-color: rgba(255, 255, 255);
      padding: 0;
      margin: 0;
      list-style: none;
      display: flex;
      flex-direction: column;
      align-items: center;
      opacity: 0;
      visibility: hidden;
      transition: opacity 0.3s, visibility 0.3s;
      box-shadow: none;
      border-radius: 10px;
    }

    #navigation.show-menu {
      opacity: 1;
      visibility: visible;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.2);
    }

    .header-title img {
      border-radius: 10px;
    }

    .menu-item-about.active a,
    .menu-item-people.active a,
    .menu-item-publications.active a,
    .menu-item-software.active a,
    .menu-item-others.active a {
      border-radius: 10px;
    }

    #main-menu ul li a {
      display: block;
      padding: 10px 15px;
      border-radius: 10px;
      transition: background-color 0.3s, color 0.3s, border-radius 0.3s;
    }

    #main-menu ul li a:hover {
      border-radius: 10px;
    }

    #main-menu ul li a span {
      display: inline-block;
      transition: background-color 0.3s, color 0.3s, border-radius 0.3s;
    }

    #main-menu ul li a span.active {
      background-color: #e0e0e0;
      color: #000;
      border-radius: 10px;
    }

    @media (max-width: 768px) {
      p {
        font-size: calc(3vw);
      }

      h3 {
        font-size: calc(4vw);
      }

      h4 {
        font-size: calc(4vw);
      }

      h2 {
        font-size: calc(5vw);
      }
    }
  </style>
</head>

<body class='page page-about'>
  <div class="wrapper">
    <div class='header'>
      <div class="container">
        <h3 class="header-title">
          <div class="icon-with-text">
            <a href="./index.html"><img src="./grouplogo.webp" style="max-width: 50%"></a>
            <div class="text">
              <h6>
                <b>
                  <font color=Black>Let's do something</font>
                  <font color=DarkRed>right!</font>
                </b>
              </h6>
              <h6>
                <b>
                  <font color=Black>Let's do something</font>
                  <font color=DarkRed>big!</font>
                </b>
              </h6>
              <h6>
                <b>
                  <font color=Black>Let's do something</font>
                  <font color=DarkRed>unique!</font>
                </b>
              </h6>
            </div>
          </div>
        </h3>
        <div id="main-menu" class="main-menu">
          <ul>
            <li class="menu-item-about">
              <a href="./index.html">
                <span>Home</span>
              </a>
            </li>
            <li class="menu-item-people">
              <a href="./people.html">
                <span>People</span>
              </a>
            </li>
            <li class="menu-item-publications">
              <a href="./publication.html">
                <span>Publication</span>
              </a>
            </li>
            <li class="menu-item-software   active">
              <a href="./research.html">
                <span>Research</span>
              </a>
            </li>
            <li class="menu-item-software">
              <a href="./info.html">
                <span>Demos</span>
              </a>
            </li>
          </ul>
        </div>
        <div class="hamburger hamburger--slider" tabindex="0" aria-label="Menu" role="button"
          aria-controls="navigation">
          <div class="hamburger-box">
            <div class="hamburger-inner"></div>
          </div>
          <nav id="navigation">
            <a href="./index.html">
              <span>Home</span>
            </a>
            <a href="./people.html">
              <span>People</span>
            </a>
            <a href="./publication.html">
              <span>Publication</span>
            </a>
            <a href="./research.html">
              <span>Research</span>
            </a>
            <a href="./info.html">
              <span>Demos</span>
            </a>
          </nav>
        </div>
      </div>
    </div>

    <div class="container mt-2">
      <div style="text-align: center;">
        <h2>
          <strong>The development of AI is inherently interconnected with
            <font color=DarkRed>human factors</font>
          </strong>
        </h2>
        <a class="img-fluid"><img src="./img/research/human-like task.webp" width="70%"></a>
      </div>
      <div>
        <p>
          Our previous research has primarily been dedicated to evaluating and exploring machine vision
          intelligence. This research encompasses various aspects such as task modeling, environment construction,
          evaluation technique, and human-machine comparisons. We strongly hold the belief that <b>
            <font color=DarkRed>the development of AI is inherently interconnected with human factors</font>
          </b>. Hence, drawing inspiration from the renowned <b>
            <font color=DarkRed>Turing Test</font>
          </b>, we have focused our investigation on the concept of <b>
            <font color=DarkRed>Visual Turing Test</font>
          </b>, aiming to integrate human elements into the evaluation of dynamic visual tasks. The ultimate goal of
          our previous work is to assess and analyze machine vision intelligence by benchmarking against human
          abilities. We believe that effective evaluation techniques are the foundation for helping us achieve
          trustworthy and secure artificial general intelligence. The following are several key aspects:
        </p>
        <h3>
          <strong>What are the abilities of humans? Designing more human-like tasks.</strong>
        </h3>
        <p>
          Focusing on utilizing Visual Object Tracking (VOT) as a representative task to explore
          dynamic visual abilities. VOT holds a pivotal role in computer vision; however, its original definition
          imposes excessive constraints that hinder alignment with human dynamic visual tracking abilities. To
          address this problem, we adopted a humanoid modeling perspective and expanded the original VOT definition.
          By eliminating the presumption of continuous motion, we introduced a more humanoid-oriented Global
          Instance Tracking (GIT) task. This expansion of the research objectives transformed VOT from a <b>
            <font color=DarkRed>perceptual level</font>
          </b>, which involves locating targets in short video sequences through visual feature contrasts, to a
          <b>
            <font color=DarkRed>cognitive level</font>
          </b> that addresses the continuous localization of targets in long videos without presuming continuous
          motion. Building upon this, we endeavored to incorporate semantic information into the GIT task and
          introduced the Multi-modal GIT (MGIT) task. The goal is to integrate a human-like understanding of long
          videos with hierarchically structured semantic labels, thereby further advancing the research objectives
          to include <b>
            <font color=DarkRed>visual reasoning</font>
          </b> within complex spatio-temporal causal relationships.
        </p>
      </div>
      <hr>
      <div>
        <h3><strong>What are the living environments of humans? Constructing more comprehensive and realistic
            datasets.</strong>
        </h3>
        <p>
          The environment in which humans reside is characterized by complexity and constant change. However,
          current research predominantly employs static and limited datasets as closed experimental environments.
          These toy examples fail to provide machines with authentic human-like visual intelligence. To address this
          limitation, we draw inspiration from film theory and propose a framework for decoupling video narrative
          content. In doing so, we have developed VideoCube, the largest-scale object tracking benchmark. Expanding
          on this work, I integrate diverse environments from the field of VOT to create SOTVerse, a dynamic and
          open task space comprising 12.56 million frames. Within this task space, researchers can efficiently
          construct different subspaces to train algorithms, thereby improving their <b>
            <font color=DarkRed>visual generalization</font>
          </b> across
          various scenarios. Furthermore, our research also focuses on <b>
            <font color=DarkRed>visual robustness</font>
          </b>. Leveraging a bio-inspired
          flapping-wing drone developed by our team, we establish the first flapping-wing drone-based benchmark
          named BioDrone to enhance visual robustness in challenging environments.
        </p>
      </div>
      <hr>
      <div>
        <h3><strong>How significant is the disparity between human and machine dynamic vision abilities? Utilizing
            human abilities as a baseline to evaluate machine intelligence.</strong></h3>
        <p>
          Computer scientists typically use large-scale datasets to evaluate machine models, while neuroscientists
          typically employ simple experimental environments to evaluate human subjects. This discrepancy makes it
          challenging to integrate human-machine evaluation into a unified framework for comparison and analysis. To
          address the aforementioned issues (How significant is the disparity between human and machine dynamic
          vision abilities?), we construct an experimental environment based on SOTVerse to enable a fair comparison
          between human and machine dynamic visual abilities. These sequences provide a thorough examination of the
          perceptual abilities, cognitive abilities, and robust tracking abilities of humans and machines. Based on
          this foundation, a human-machine dynamic visual capability evaluation framework is designed. Finally, a
          fine-grained experimental analysis is carried out from the perspectives of human-machine comparison and
          human-machine collaboration. The experimental results demonstrate that representative tracking algorithms
          have gradually narrowed the gap with human subjects. Furthermore, both humans and machines exhibit unique
          strengths in dynamic visual tasks, suggesting significant potential for human-machine collaboration.
        </p>
      </div>
      <hr>
      <div style="text-align: center;">
        <a class="img-fluid"><img src="./img/research/3E.webp" width="90%"></a>
      </div>
      <div>
        <h3><strong>3E paradigm: Environment, Evaluation and Executor.</strong></h3>
        <p>
          This human-centered evaluation concept is referred to as Visual Turing Test, and we have presented our
          thoughts and future prospects in this direction through a comprehensive review on intelligent evaluation
          techniques. These research contents can be summarized using the 3E paradigm. In order to enable machines
          to acquire human <b>
            <font color=DarkRed>abilities</font>
          </b>, we need to construct a humanoid proxy <b>
            <font color=DarkRed>task</font>
          </b> and execute it through interactions
          among the <b>
            <font color=DarkRed>environment</font>
          </b>, <b>
            <font color=DarkRed>evaluation</font>
          </b>, and <b>
            <font color=DarkRed>executors</font>
          </b>. Ultimately, the executors’ performance reflects their
          level of ability, and their upper limit of ability is continuously improved through ongoing iterations. We
          hope these research can create a comprehensive system that lays a solid research foundation for improving
          the dynamic visual abilities of machines.
        </p>
      </div>
    </div>
  </div>

  <script>
    document.addEventListener('DOMContentLoaded', function () {
      var menuButton = document.querySelector('.hamburger');
      var navigation = document.getElementById('navigation');

      menuButton.addEventListener('click', function () {
        navigation.classList.toggle('show-menu');
      });
    });
  </script>
  <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
    integrity="sha256-pasqAKBDmFT4eHoN2ndd6lN370kFiGUFyTiUHWhU7k8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.15.0/umd/popper.min.js"
    integrity="sha256-fTuUgtT7O2rqoImwjrhDgbXTKUwyxxujIMRIK7TbuNU=" crossorigin="anonymous"></script>
</body>

</html>